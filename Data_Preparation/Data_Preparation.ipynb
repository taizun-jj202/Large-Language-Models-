{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5be137",
   "metadata": {},
   "source": [
    "#### Step 1 : Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c8ed30c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters in file : 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total Characters in file :\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb47158",
   "metadata": {},
   "source": [
    "Goal is to tokenize all 20,479 words into tokens, assign token IDs to these tokens and convert these tokens/token IDs into vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec4ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1403322b",
   "metadata": {},
   "source": [
    "Testing word separation schema on small subset of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1b632a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', ',', '', ' ', 'Is', ' ', 'this', '--', '', ' ', '', ' ', 'a', ' ', 'test', '?', '']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "test_text = \"Hello, world, Is this--  a test?\" \n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', test_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef387571",
   "metadata": {},
   "source": [
    "Applying word separation(pre-processing) schema to file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286dae31",
   "metadata": {},
   "source": [
    "Tokenize entire short story :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "306422d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', ' ', 'good']\n",
      "9235\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "print(preprocessed[:25])\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd83fd",
   "metadata": {},
   "source": [
    "#### Step 2 : Creating Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8073491c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2bf63762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "\n",
    "# for i, item in enumerate(vocab.items()):\n",
    "#     print(i,item)\n",
    "#     if i >= 45:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4782fc",
   "metadata": {},
   "source": [
    "Tokenizer Class that has encode and decode methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "09c710bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer : \n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Pre-processed text. \n",
    "        # i.e sentences that are broken up into words.\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        token_ids = [self.str_to_int[strngs] for strngs in preprocessed]\n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        text = \"\".join([self.int_to_str[ints] for ints in token_ids])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7a366b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "It was not till three years later that, in the course of a few weeks' idling on the Riviera\n",
      "[59, 2, 1080, 2, 714, 2, 1013, 2, 1007, 2, 1126, 2, 607, 2, 990, 8, 0, 2, 571, 2, 991, 2, 300, 2, 725, 2, 118, 2, 440, 2, 1088, 5, 0, 2, 568, 2, 730, 2, 991, 2, 87]\n",
      "\n",
      "\n",
      "Token ID back to text : \n",
      "It was not till three years later that, in the course of a few weeks' idling on the Riviera\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer class\n",
    "# Make sure not to overwrite the class name 'Tokenizer'\n",
    "tokenizer_instance = Tokenizer(vocab)  # Use a different variable name for the instance\n",
    "\n",
    "test_text = \"It was not till three years later that, in the course of a few weeks' idling on the Riviera\"\n",
    "\n",
    "\n",
    "# Use the instance to encode and decode\n",
    "token_ids_for_test_text = tokenizer_instance.encode(test_text)\n",
    "print(\"\\n\")\n",
    "print(test_text)\n",
    "print(token_ids_for_test_text)\n",
    "print(\"\\n\")\n",
    "print(\"Token ID back to text : \")\n",
    "tokenid_to_text = tokenizer_instance.decode(token_ids_for_test_text)\n",
    "print(tokenid_to_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda7b01",
   "metadata": {},
   "source": [
    "Adding End-of-Text and UNK tokens to extend vocabulary in case where we encounter a unseen word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9876cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend(([\"<|endoftext|>\", \"<|unk|>\"]))\n",
    "\n",
    "vocab2 = {token:integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9b0d7183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1130)\n",
      "('your', 1131)\n",
      "('yourself', 1132)\n",
      "('<|endoftext|>', 1133)\n",
      "('<|unk|>', 1134)\n"
     ]
    }
   ],
   "source": [
    "len(vocab2)\n",
    "\n",
    "for i, item in enumerate(list(vocab2.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a387978",
   "metadata": {},
   "source": [
    "Extending Tokenizer Class to include UNK and ENDOFTEXT tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6b692774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV2 :\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode (self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "        token_ids = [self.str_to_int[strngs] for strngs in preprocessed]\n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        text = \"\".join([self.int_to_str[ints] for ints in token_ids])\n",
    "        return text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3f75b082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea ?<|endoftext|>In the sunlit terraces of the palace\n"
     ]
    }
   ],
   "source": [
    "tokenizerV2_instance = TokenizerV2(vocab2)  # Use a different variable name for the instance\n",
    "\n",
    "text1 = \"Hello, do you like tea ?\"\n",
    "text2 = \"In the sunlit terraces of the palace\"\n",
    "text = \"<|endoftext|>\".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8bc6abab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea ?<|endoftext|>In the sunlit terraces of the palace [1134, 8, 0, 2, 358, 2, 1129, 2, 631, 2, 978, 2, 0, 13, 1134, 2, 991, 2, 959, 2, 987, 2, 725, 2, 991, 2, 1134]\n",
      "<|unk|>, do you like tea ?<|unk|> the sunlit terraces of the <|unk|>\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizerV2_instance.encode(text)\n",
    "print(text, encoded)\n",
    "decoded = tokenizerV2_instance.decode(encoded)\n",
    "print(decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
